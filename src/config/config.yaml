apps:
  cab:
    category: cab
    validation:
      name_match_threshold: 75
      address_match_threshold: 40
  meal:
    category: meal
    validation:
      vendor_match_threshold: 60
  fuel:
    category: fuel
    validation:
      name_match_threshold: 75

paths:
  clients_file: clients.json
  resources_dir: resources
  processed_dir: resources/processed_inputs   # local sync writes here; app can use --resources-dir <this>
  output_dir: src/model_output

# SharePoint sync (used by scripts/sync_sharepoint_to_resources.py)
# Credentials: set env SHAREPOINT_USERNAME, SHAREPOINT_PASSWORD (required for user auth).
# Overrides: SHAREPOINT_SITE_URL and SHAREPOINT_ROOT override site_url and root_folder below.
sharepoint:
  site_url: ""                    # e.g. https://yourtenant.sharepoint.com/sites/YourSite
  root_folder: ""                 # server-relative path, e.g. /sites/YourSite/Shared Documents/Bills
  default_client: "unknown"       # used when client cannot be inferred from folder name
  default_month: "unknown"       # for local mode when month not in path
  employee_mapping_file: ""       # optional JSON path (project-relative): {"Display Name": "IIIPL-1234", ...}
  # Categories synced into paths.resources_dir: commute, meal, fuel (folder names normalized to {emp_id}_{emp_name}_{month}_{client})
  categories:
    commute:
      keywords: ["cab", "taxi", "commute", "ride", "uber", "ola", "transport"]
    meal:
      keywords: ["meal", "meals", "food", "lunch", "dinner"]
    fuel:
      keywords: ["fuel", "petrol", "diesel", "gas"]
  # Map path substring (lowercase) -> client id for folder naming
  client_keywords:
    tesco: "tesco"
    amex: "amex"
    "american express": "amex"

validation:
  name_match_threshold: 75
  address_match_threshold: 40
  manual_id_prefix: "MANUAL"
  date_format: "%d/%m/%Y"
  # Default per-bill cap (overridden by apps.meal/fuel.validation.amount_limit_per_bill). Null = no cap.
  amount_limit_per_bill: null

# Folder name parsing: {emp_id}_{emp_name}_{month}_{client}
folder_parser:
  separator: "_"
  min_parts: 4

# Bill file extensions when scanning folders
folder:
  bill_extensions: [".pdf", ".png", ".jpg", ".jpeg"]

# OCR (Tesseract) settings
ocr:
  tesseract:
    dpi: 300
    lang: "eng"

# LLM: change provider by setting 'provider' to one of the keys under 'providers'.
# API key: set env var (api_key_env) and/or api_key here. Env takes precedence.
# Use provider: ollama for local models (no API key; run scripts/run_local_llm.sh first).
llm:
  provider: ollama
  temperature: 0
  default_model: llama3.2
  providers:
    ollama:
      model: llama3.2              # e.g. llama3.2, mistral, codellama
      base_url: "http://localhost:11434"   # Ollama API (optional; default localhost:11434)
    groq:
      model: llama-3.3-70b-versatile
      api_key_env:    # env var name; key can also be set here as api_key
      api_key: ""                  # optional: set key in config (prefer env for security)
    openai:
      model: gpt-4o-mini
      api_key_env: 
      api_key: 
    anthropic:
      model: claude-3-5-sonnet-20241022
      api_key_env: ANTHROPIC_API_KEY
      api_key: ""
    azure:
      model: gpt-4   # deployment name in Azure
      api_key_env: AZURE_OPENAI_API_KEY
      api_key: ""
      api_base_env: AZURE_OPENAI_ENDPOINT   # e.g. https://your-resource.openai.azure.com/
      api_version: "2024-02-15-preview"

# Optional org API: employee details, leave, manager. Not mandatory; if disabled or API fails, flow continues.
org_api:
  enabled: false
  base_url: ""                    # e.g. https://your-org-api.example.com
  api_key_env: ORG_API_KEY        # env var for Bearer token (optional)
  employee_path: "/api/employees/{employee_id}"   # path template; {employee_id} is replaced
  timeout: 10

# RAG Configuration for Policy Extraction
# Set enabled: true to use vector-based semantic search for policy queries
rag:
  enabled: false                    # Toggle RAG on/off (default: false)
  chunk_size: 500                   # Size of text chunks for vectorization
  chunk_overlap: 50                 # Overlap between chunks
  top_k: 5                          # Number of relevant chunks to retrieve
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"  # HuggingFace embedding model
